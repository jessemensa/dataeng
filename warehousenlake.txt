BUILDING A DATAWAREHOUSE IN GOOGLE CLOUD PLATFORM 1 
INTRODUCTION TO DATA ENGINEERING 
BUILDING A DATA WAREHOUSE 
BUILDING A DATA LAKE 


BUILDING A DATAWAREHOUSE IN GOOGLE CLOUD PLATFORM 
TOOLS TO BE USED 
1. BIGQUERY 
2. CLOUD SQL 
3. GOOGLE CLOUD STORAGE 

BIGQUERY FACTS 
1. STORES DATA IN A DISTRIBUTED FILESYSTEM CALLED GOOGLE COLOSUIS 
IT WAS INSPIRED FOR HADOOP FILE SYSTEM 
ACCESS DATA VIA SQL INTERFACE 
BIGQUERY PROCESSES DATA IN A DISTRIBUTED SQL EXECUTION ENGINE 

BIGQUERY DATA LOCATION 
PHYSICALLY LOCATED IN DIFFERENT COUNTRIES AND CITIES 
GROUPED INTO REGIONS AND EACH REGION HAS ZONES 
IE SAY SOUTH EAST ASIA 
TWO ZONES CAN BE SINGAPORE AND JAKARTA 
WHICH I CHOOSE JAKARTA AS BIGQUERY DATASET LOCATION 
DATA IS STORED AND PROCESSED IN JAKARTA CLUSTERS 
BigQuery is very good at storing large volumes of data and performing analysis 
and processing immediately using the SQL interface.


CREATING A DATASET IN BIGQUERY 
EXAMPLE 
REPAIRS 
TENANT SURVEYS 
ARREARS 

PREPARING FOR DEVELOPING DATAWAREHOUSE 
WE WILL NEED THE FOLLOWING 
1. CLOUD SHELL 
2. CHECK CREDENTIALS USING gcloud info 
3. INITIALISE OUR CREDENTIALS USING gcloud init 
4. WRITE CODE AND DATASETS 
5. UPLOAD DATA FROM GCS from git 

gcloud info & gcloud init command 

gcloud info => checks the current setupv
1. summary of installed compoenents, versions and configurations of google cloud sdk in system 
2. It also lists the directories in your system's PATH environment variable,
3. It looks like you have the Cloud SDK installed and configured, and that you have specified a project 
(engexamspreparation) to use with the SDK.

gcloud init ?? 
a command that you can use to initialize the Google Cloud SDK on your local machine.
it will prompt you to authenticate with your Google account, select a Google Cloud project to use, 
and set the default configuration for the Cloud SDK.

observations 
gcloud init 
=> create a new configuration (configuration name set it to personal-config)
=> log into your account 
=> choose email address used to sign in into google cloud or log in with new account 
=> [1] 
=> choose cloud project to use 
=> [1] 
=> done 

TASKS 
DEVELOPING A DATA WAREHOUSE W DOCUMENTATION  

SCENARIOS 
1. Maria wants to know the top two region IDs, ordered by the total capacity of the stations in that region.
2. Wants be able to download the answers to my questions as CSV files to my local computer.
3. The data source table is the station table, which is located in the CloudSQL-MySQL database.

HOW TO THINK IN SUCH SITUATIONS 
1. WHAT WILL YOU DO 
2. WHAT GCP SERVICES WILL YOU USE ?? 
3. HOW WILL YOU DO IT ? 

PLANNING 
1. SINCE THERE IS A SPECIFIC BUSINESS RULE, WHICH IS RANKING AND CALCULATING TOTAL CAPACITY 
THERE IS THE NEED FOR SOME TRANSFORMATIONS. 

NOW STARTING FROM SCRATCH 
1. CREATE A MYSQL DATABASE -> EXTRACT MYSQL TO GCS -> LOAD GCS TO BIGQUERY -> CREATE A BIGQUERY DATA MART 

NEED TO CREATE A MYSQL DATABASE IN CLOUD SQL 
FIRST STEP => PREPARE A CLOUD SQL ENV 
1. CREATE A CLOUD SQL INSTANCE 
2. CONNECT TO MYSQL INSTANCE 
3. CREATE MYSQL DATABASE 
4. CREATE A TABLE IN MYSQL DATABASE 
5. IMPORT CSV DATA INTO MYSQL DATABASE 

STEP 1 
gcloud sql instances create: This is the command to create a new Cloud SQL instance.
mysql-instance-source: This is the name of the new Cloud SQL instance.
database-version=MYSQL_5_7: This specifies the version of the MySQL database that will be used for the instance.
tier=db-g1-small: This specifies the tier of the Cloud SQL instance.
region=us-central1: This specifies the region where the Cloud SQL instance will be created.
root-password=exper123: This sets the root password for the instance.
availability-type=zonal: This specifies the availability type for the instance.
storage-size=10GB: This specifies the storage size for the instance.
Storage-type=HDD: This specifies the storage type for the instance.

gcloud sql instances create mysql-instance-source \ 
--database-version=MYSQL_5_7 \ 
--tier=db-g1-small \ 
--region=us-central1 \ 
--root-password=packt123 \ 
--availability-type=zonal \ 
--storage-size=10GB \ 
--storage-type=HDD 

RUN THIS IN CLOUD SHELL ABOVE 
WAIT FOR ABOUT 5 MINS FOR IT TO FINISH 

NOTES: WE WILL BE USING GITHUB AS A WAY TO ALSO LEARN AND UNDERSTAND GITHUB 
WE HAVE UPLOADED THIS PROJECT INTO A GIT REPO USING THESE STEPS 
git init 
git add . 
git commit -m "Initial commit" 
git remote add origin  https://github.com/jessemensa/dataeng.git
git push -u main origin 

to update whatever is here to giithub right now 
use 
1. check git status 
git status 
git add . 
git commit -m "current changes name"
git push -u origin 

UPDATE CHANGES LOCALLY 
git pull origin main 


STEP 2 
WE WILL CONNECT THIS TO A MYSQL INSTANCE 
gcloud sql connect mysql-instance-source --user=root
NOTE: WILL BE PROMPTED FOR PASSWORD: packt123 

STEP 3: WE ARE INSIDE MYSQL INSTANCE 
we need to create a database that will hold our data 
CREATE DATABASE apps_db; 
CREATE TABLE INSIDE THE DATABASE 
CREATE TABLE apps_db.stations(
    station_id varchar(255), 
    name varchar(255), 
    region_id varchar(10), 
    capacity integer 
); 

NOW LETS UPLOAD DATA TO BE USED 
FIRST WE WILL UPDATE THE GITHUB REPO WITH UPDATED DATA 
IT HAS BEEN UPDATED 
NOW NEED TO UPLOAD LOCAL FILE TO GOOGLE CLOUD STORAGE USING gsutil 

STEPS 
1. FIRST NEED TO CREATE A CLOUD STORAGE BUCKET 
2. LOCATION WILL BE US(MULTIPLE REGIONS IN UNITED STATES) WHICH IS BY DEFAULT 
THERE ARE STORAGE DEFAULT CLASSES WHEN CREATING A BUCKET 
1. STANDARD 2. NEARLINE 3. COLDLINE AND 4. ARCHIVE 
3. CHOOSE NEXT TILL ITS ALL CREATED 

NOW AFTER ITS CREATED 
RUN THESE COMMANDS IN CLOUD SHELL 
export DESTINATION_BUCKET_NAME=engexamspreparation-bucket
gsutil cp -r datawarehouse/dataset/* gs://$DESTINATION_BUCKET_NAME

TO CONNECT TO MYSQL INSTANCE AGAIN 
=> gcloud sql connect mysql-instance-source --user=root


NOW LETS UPLOAD STATION FILE IN GOOGLE CLOUD STORAGE INTO CLOUD SQL CONSOLE 
1. CLICK INSIDE THE CREATED CLOUD SQL CONSOLE 
2. CHOOSE THE NAME OF THE DATA FILE IN GCS BUCKET => file name is stations.csv 
3. CHANGE THE FILE FORMAT OPTION TO CSV 
4. INPUT THE DESTINATION DATABASE apps_db and table name is stations 
5. ONCE EVERYTHING IS DONE, CLICK IMPORT 

RETURN TO CLOUD SHELL 
RUN A SELECT STATEMENT => SELECT * FROM apps_db.stations LIMIT 10;
MYSQL > exit


PART 2 => EXTRACT DATA FROM MYSQL TO GCS 
NOW USE GCLOUD COMMAND TO DUMP THE FILES INTO GCS 

FIRST OF ALL 
1. WE NEED TO HANDLE IDENTITY AND ACCESS MANAGEMENT(IAM) 
WE NEED TO ASSIGN CLOUDSQL SERVICE ACCOUNT A STORAGE OBJECT ADMIN ROLE FIRST 

LETS DETOUR AND TALK ABOUT IAM 
IAM IS A BROAD CONCEPT BY ITSELF AND GCP USES IAM TO MANAGE USER AUTHENTICATION AND AUTHORISATION 
OF ALL SERVICES. 

WE NEED TO GIVE CLOUD SQL A STORAGE OBJECT ADMIN ROLE(WHICH ALLOWS IT TO LOAD DATA INTO GOOGLE CLOUD STORAGE) 

NAVIGATE TO IAM 
1. IAM 
2. CLICK GRANT ACCESS 
3. PASTE THE CLOUD SQL SERVICE ACCOUNT INTO NEW PRINCIPLES 
4. THEN SELECT A ROLE 


NOW NAVIGATE TO CLOUD SHELL AGAIN 
EDITOR => CREATE A FILE THAT LOADS DATA TO CLOUD STORAGE 
cloudstoragebucketname = engexamspreparation-bucket

gcloud sql export csv mysql-instance-source \ 
gs://$cloudstoragebucketname/mysql_export/stations/20180101/stations.csv \ 
--database=apps_db \ 
--offload \ 
--query='SELECT * FROM stations WHERE station_id <= 200;' 

gcloud sql export mysql-instance-source \ 
gs://$cloudstoragebucketname/mysql_export/stations/20180102/stations.csv \ 
--database=apps_db \ 
--offload \ 
-- query='SELECT * FROM stations WHERE station_id <= 400;'

EXPLAINED CODE ABOVE 
gcloud sql export csv => exports data from cloud sql instance to a csv file 
mysel-instance-source => specifies the name of the cloud sql instance 
gs://$cloudstoragebucketname/mysql_export/stations/20180101/stations.csv \ 
=> location in cloud storage where the csv file will be created 
and specifying the cloud storage bucket 
--database=apps_db \ 
=> name of the database within cloud sql to be exported out of 
-- offload \ => indicates the export should be done using a managed cloud sql export service 
which can improve performance and reduce the load on the databases. 
-- query => select rows that should be exported 


IN CLOUD SHELL 
NAVIGATE TO THE dataeng folder 
navigate to datawarehouse folder 
navigate to code folder 
run file 
=> sh cloudsqltocloudstorage.sh 

AP PROBLEM I FACE WAS 
=> error: Your local changes to the following files would be overwritten by merge:
        datawarehouse/code/cloudsqltocloudstorage.sh

SOLUTION 
git reset --hard
git pull 

MAKE SURE TO DELETE SQL CLOUD INSTANCE WHEN DONE USING IT OR IT WILL RUN 
COMMAND LINE 
#gcloud sql instances delete mysql-instance-source


PART 3 
LOAD GCS TO BIGQUERY 
USING BIGQUERY NOW, CREATE A NEW TABLE FROM THE BIGQUERY CONSOLE 
NAME THAT TAKE raw_bikesharing 
inside the dataset create a table and pick from 
mysql_export/stations/20180101/stations.csv 
table name is stations 
schema option is Edit as text 
write the following in the schema textbox 
station_id:STRING,name:STRING,region_id:STRING,capacity:INTEGER 
create table 
NOTE: CHECK THE STATIONS TABLE TO SEE IF THE TABLE EXISTS NOW 

Create a BigQuery data mart 
business users will access data from data marts 
create a new dataset called name dm_regional_manager
TASK: BUSINESS USER, WANTS TO KNOW THE TOP TWO REGION IDs ordered by total stations capacity in that region. 

Now two options for the query results 
1. Create a table 
2. Create a view 

both tables and views have merits and demerits 
two merits of using a view 
1. View costs no additional storage, just saves SQL formula 
2. Real time, if you access the view then every time the underlying table changes, the view will get the latest update.

drawbacks to views 
1. views can be heavy,and when the underlying tables are large and there are many joins
and aggregations in the view's query, you may end up having very heavy processing

CONSIDER THIS SCENARIO 
Imagine you have 5 upstream raw tables, each 1 PB in size, and your downstream consists of 1,000 views 
accessing the 5 tables. You may end up processing the PBs of data repeatedly, and that's bad in terms of 
both cost and performance.

HOWEVER IN THIS SCENARIO => THE TABLE IS SMALL SO USE VIEWS 
CHECK createview.sql and follow instructions 
save the results of the query in csv file 


DATAWAREHOUSE IN BIGQUERY 
SCENARIO 2 
SOME REQUIREMENTS 
Access the following information 
1. How many bike trips take place daily? 
2. what is the daily average trip duratoon ? 
3. The top 5 station names as the starting station that has the longest trip duration 
4. The top 5 region names that have the shortest total trip durations 

THE BIKE TRIPS DATA IS IN THE GCS BUCKET AND EACH BUCKET FOLDER CONTAINS DAILY DATA 
THE REGIONS DATA IS FROM THE BIGQUERY PUBLIC DATASET 
NEW DATA WILL BE UPDATED DAILY IN THE GCS BUCKET FOR stations and trips table 

NOW ASK THE SAME QUESTIONS 
1. WHAT WILL YOU DO ? 
2. WHAT SERVICES WILL YOU USE ? 
3. HOW WILL YOU DO IT ?

IN THIS SCENARIO, WE WILL BE USING THE PYTHON API INSTEAD OF USING THE GCP CONSOLE 
CONSOLE IS NOT SCALABLE IN ENGINEERING PERSPECTIVE 
IE NEED TO CREATE MULTIPLE ETL PIPELINES 
BETTER TO HAVE LOOPS IN THE PIPELINES USING CODE 
ALSO MAKES IT EASY TO TEST AND DEPLOY 


SOME INITIAL THOUGHTS AND PLANNING 
1. SINCE THE USER IS INTERESTED IN DAILY MEASUREMENTS, WE WILL CREATE A LAYER FOR DAILY AGGREGATIONS TO THE USER. 
2. THERE WILL BE NEW DATA DAILY, SO WE NEED TO PLAN HOW TO HANDLE THE INCOMING DATA FROM GCS DIRECTORIES 
3. IMAGINE IF THREE TABLES, STATIONS, REGIONS, AND TRIPS IN REAL LIFE, IF THEY ARE DIFFERENT ENTITIES 
STATIONS AND REGIONS ARE STATIC OBJECTS WHILE TRIPS ARE EVENTS 
4. THE DATAMART IS SIMILAR TO SCENARIO 1 

FIVE PRINCIPAL STEPS 
1. CREATE REQUIRED DATASETS 
2. LOAD THE INITIAL TRIPS AND REGION TABLES TO BIGQUERY: 
--- TRIPS FROM GCS 
--- REGIONS FROM THE BIGQUERY PUBLIC DATASET 
3. HANDLE THE DAILY BATCH DATA LOADING 
--- FOR TRIPS TABLE 
--- FOR STATIONS TABLE 
4. DESIGN DATA MODELLING FOR BIGQUERY 
5. STORE THE BUSINESS QUESTIONS RESULT IN TABLES 
THERE ARE TWO DIFFERENT DATES IN THIS. 
INITIAL LOAD IS USING 2018-01-01 
HANDLE NEW DATA FROM 2018-01-02 AND UPCOMING DAYS WITHOUT ANY ISSUES 

LETS CREATE THE DATASETS USING PYTHON 
1. add a new dataset called dwh_bikesharing 
2. add other datasets => raw_bikesharing and dm_bikesharing 
(name the file createdatasets.py) and check the file 
upload the code to github dn update the codebase in editor and run the code 

NOW LOADING THE TRIPS INTO BIGQUERY 
=> create a python file to load the trips data 
=> loadtripsdata.py 