BUILDING A DATAWAREHOUSE IN GOOGLE CLOUD PLATFORM 1 
INTRODUCTION TO DATA ENGINEERING 
BUILDING A DATA WAREHOUSE 
BUILDING A DATA LAKE 


BUILDING A DATAWAREHOUSE IN GOOGLE CLOUD PLATFORM 
TOOLS TO BE USED 
1. BIGQUERY 
2. CLOUD SQL 
3. GOOGLE CLOUD STORAGE 

BIGQUERY FACTS 
1. STORES DATA IN A DISTRIBUTED FILESYSTEM CALLED GOOGLE COLOSUIS 
IT WAS INSPIRED FOR HADOOP FILE SYSTEM 
ACCESS DATA VIA SQL INTERFACE 
BIGQUERY PROCESSES DATA IN A DISTRIBUTED SQL EXECUTION ENGINE 

BIGQUERY DATA LOCATION 
PHYSICALLY LOCATED IN DIFFERENT COUNTRIES AND CITIES 
GROUPED INTO REGIONS AND EACH REGION HAS ZONES 
IE SAY SOUTH EAST ASIA 
TWO ZONES CAN BE SINGAPORE AND JAKARTA 
WHICH I CHOOSE JAKARTA AS BIGQUERY DATASET LOCATION 
DATA IS STORED AND PROCESSED IN JAKARTA CLUSTERS 
BigQuery is very good at storing large volumes of data and performing analysis 
and processing immediately using the SQL interface.


CREATING A DATASET IN BIGQUERY 
EXAMPLE 
REPAIRS 
TENANT SURVEYS 
ARREARS 

PREPARING FOR DEVELOPING DATAWAREHOUSE 
WE WILL NEED THE FOLLOWING 
1. CLOUD SHELL 
2. CHECK CREDENTIALS USING gcloud info 
3. INITIALISE OUR CREDENTIALS USING gcloud init 
4. WRITE CODE AND DATASETS 
5. UPLOAD DATA FROM GCS from git 

gcloud info & gcloud init command 

gcloud info => checks the current setupv
1. summary of installed compoenents, versions and configurations of google cloud sdk in system 
2. It also lists the directories in your system's PATH environment variable,
3. It looks like you have the Cloud SDK installed and configured, and that you have specified a project 
(engexamspreparation) to use with the SDK.

gcloud init ?? 
a command that you can use to initialize the Google Cloud SDK on your local machine.
it will prompt you to authenticate with your Google account, select a Google Cloud project to use, 
and set the default configuration for the Cloud SDK.

observations 
gcloud init 
=> create a new configuration (configuration name set it to personal-config)
=> log into your account 
=> choose email address used to sign in into google cloud or log in with new account 
=> [1] 
=> choose cloud project to use 
=> [1] 
=> done 

TASKS 
DEVELOPING A DATA WAREHOUSE W DOCUMENTATION  

SCENARIOS 
1. Maria wants to know the top two region IDs, ordered by the total capacity of the stations in that region.
2. Wants be able to download the answers to my questions as CSV files to my local computer.
3. The data source table is the station table, which is located in the CloudSQL-MySQL database.

HOW TO THINK IN SUCH SITUATIONS 
1. WHAT WILL YOU DO 
2. WHAT GCP SERVICES WILL YOU USE ?? 
3. HOW WILL YOU DO IT ? 

PLANNING 
1. SINCE THERE IS A SPECIFIC BUSINESS RULE, WHICH IS RANKING AND CALCULATING TOTAL CAPACITY 
THERE IS THE NEED FOR SOME TRANSFORMATIONS. 

NOW STARTING FROM SCRATCH 
1. CREATE A MYSQL DATABASE -> EXTRACT MYSQL TO GCS -> LOAD GCS TO BIGQUERY -> CREATE A BIGQUERY DATA MART 

NEED TO CREATE A MYSQL DATABASE IN CLOUD SQL 
FIRST STEP => PREPARE A CLOUD SQL ENV 
1. CREATE A CLOUD SQL INSTANCE 
2. CONNECT TO MYSQL INSTANCE 
3. CREATE MYSQL DATABASE 
4. CREATE A TABLE IN MYSQL DATABASE 
5. IMPORT CSV DATA INTO MYSQL DATABASE 

STEP 1 
gcloud sql instances create: This is the command to create a new Cloud SQL instance.
mysql-instance-source: This is the name of the new Cloud SQL instance.
database-version=MYSQL_5_7: This specifies the version of the MySQL database that will be used for the instance.
tier=db-g1-small: This specifies the tier of the Cloud SQL instance.
region=us-central1: This specifies the region where the Cloud SQL instance will be created.
root-password=exper123: This sets the root password for the instance.
availability-type=zonal: This specifies the availability type for the instance.
storage-size=10GB: This specifies the storage size for the instance.
Storage-type=HDD: This specifies the storage type for the instance.

gcloud sql instances create mysql-instance-source \ 
--database-version=MYSQL_5_7 \ 
--tier=db-g1-small \ 
--region=us-central1 \ 
--root-password=packt123 \ 
--availability-type=zonal \ 
--storage-size=10GB \ 
--storage-type=HDD 

RUN THIS IN CLOUD SHELL ABOVE 
WAIT FOR ABOUT 5 MINS FOR IT TO FINISH 

NOTES: WE WILL BE USING GITHUB AS A WAY TO ALSO LEARN AND UNDERSTAND GITHUB 
WE HAVE UPLOADED THIS PROJECT INTO A GIT REPO USING THESE STEPS 
git init 
git add . 
git commit -m "Initial commit" 
git remote add origin  https://github.com/jessemensa/dataeng.git
git push -u main origin 

to update whatever is here to giithub right now 
use 
1. check git status 
git status 
git add . 
git commit -m "current changes name"
git push -u origin 

UPDATE CHANGES LOCALLY 
git pull origin main 


STEP 2 
WE WILL CONNECT THIS TO A MYSQL INSTANCE 
gcloud sql connect mysql-instance-source --user=root
NOTE: WILL BE PROMPTED FOR PASSWORD: packt123 

STEP 3: WE ARE INSIDE MYSQL INSTANCE 
we need to create a database that will hold our data 
CREATE DATABASE apps_db; 
CREATE TABLE INSIDE THE DATABASE 
CREATE TABLE apps_db.stations(
    station_id varchar(255), 
    name varchar(255), 
    region_id varchar(10), 
    capacity integer 
); 

NOW LETS UPLOAD DATA TO BE USED 
FIRST WE WILL UPDATE THE GITHUB REPO WITH UPDATED DATA 
IT HAS BEEN UPDATED 
NOW NEED TO UPLOAD LOCAL FILE TO GOOGLE CLOUD STORAGE USING gsutil 

STEPS 
1. FIRST NEED TO CREATE A CLOUD STORAGE BUCKET 
2. LOCATION WILL BE US(MULTIPLE REGIONS IN UNITED STATES) WHICH IS BY DEFAULT 
THERE ARE STORAGE DEFAULT CLASSES WHEN CREATING A BUCKET 
1. STANDARD 2. NEARLINE 3. COLDLINE AND 4. ARCHIVE 
3. CHOOSE NEXT TILL ITS ALL CREATED 

NOW AFTER ITS CREATED 
RUN THESE COMMANDS IN CLOUD SHELL 
export DESTINATION_BUCKET_NAME=engexamspreparation-bucket
gsutil cp -r datawarehouse/dataset/* gs://$DESTINATION_BUCKET_NAME

TO CONNECT TO MYSQL INSTANCE AGAIN 
=> gcloud sql connect mysql-instance-source --user=root


NOW LETS UPLOAD STATION FILE IN GOOGLE CLOUD STORAGE INTO CLOUD SQL CONSOLE 
1. CLICK INSIDE THE CREATED CLOUD SQL CONSOLE 
2. CHOOSE THE NAME OF THE DATA FILE IN GCS BUCKET => file name is stations.csv 
3. CHANGE THE FILE FORMAT OPTION TO CSV 
4. INPUT THE DESTINATION DATABASE apps_db and table name is stations 
5. ONCE EVERYTHING IS DONE, CLICK IMPORT 

RETURN TO CLOUD SHELL 
RUN A SELECT STATEMENT => SELECT * FROM apps_db.stations LIMIT 10;
MYSQL > exit


PART 2 => EXTRACT DATA FROM MYSQL TO GCS 
NOW USE GCLOUD COMMAND TO DUMP THE FILES INTO GCS 

FIRST OF ALL 
1. WE NEED TO HANDLE IDENTITY AND ACCESS MANAGEMENT(IAM) 
WE NEED TO ASSIGN CLOUDSQL SERVICE ACCOUNT A STORAGE OBJECT ADMIN ROLE FIRST 

LETS DETOUR AND TALK ABOUT IAM 
IAM IS A BROAD CONCEPT BY ITSELF AND GCP USES IAM TO MANAGE USER AUTHENTICATION AND AUTHORISATION 
OF ALL SERVICES. 

WE NEED TO GIVE CLOUD SQL A STORAGE OBJECT ADMIN ROLE(WHICH ALLOWS IT TO LOAD DATA INTO GOOGLE CLOUD STORAGE) 

NAVIGATE TO IAM 
1. IAM 
2. CLICK GRANT ACCESS 
3. PASTE THE CLOUD SQL SERVICE ACCOUNT INTO NEW PRINCIPLES 
4. THEN SELECT A ROLE 


NOW NAVIGATE TO CLOUD SHELL AGAIN 
EDITOR => CREATE A FILE THAT LOADS DATA TO CLOUD STORAGE 
cloudstoragebucketname = engexamspreparation-bucket

gcloud sql export csv mysql-instance-source \ 
gs://$cloudstoragebucketname/mysql_export/stations/20180101/stations.csv \ 
--database=apps_db \ 
--offload \ 
--query='SELECT * FROM stations WHERE station_id <= 200;' 

gcloud sql export mysql-instance-source \ 
gs://$cloudstoragebucketname/mysql_export/stations/20180102/stations.csv \ 
--database=apps_db \ 
--offload \ 
-- query='SELECT * FROM stations WHERE station_id <= 400;'

EXPLAINED CODE ABOVE 
gcloud sql export csv => exports data from cloud sql instance to a csv file 
mysel-instance-source => specifies the name of the cloud sql instance 
gs://$cloudstoragebucketname/mysql_export/stations/20180101/stations.csv \ 
=> location in cloud storage where the csv file will be created 
and specifying the cloud storage bucket 
--database=apps_db \ 
=> name of the database within cloud sql to be exported out of 
-- offload \ => indicates the export should be done using a managed cloud sql export service 
which can improve performance and reduce the load on the databases. 
-- query => select rows that should be exported 


IN CLOUD SHELL 
NAVIGATE TO THE dataeng folder 
navigate to datawarehouse folder 
navigate to code folder 
run file 
=> sh cloudsqltocloudstorage.sh 

AP PROBLEM I FACE WAS 
=> error: Your local changes to the following files would be overwritten by merge:
        datawarehouse/code/cloudsqltocloudstorage.sh

SOLUTION 
git reset --hard
git pull 

MAKE SURE TO DELETE SQL CLOUD INSTANCE WHEN DONE USING IT OR IT WILL RUN 
COMMAND LINE 
#gcloud sql instances delete mysql-instance-source


PART 3 
LOAD GCS TO BIGQUERY 
USING BIGQUERY NOW, CREATE A NEW TABLE FROM THE BIGQUERY CONSOLE 
NAME THAT TAKE raw_bikesharing 
inside the dataset create a table and pick from 
mysql_export/stations/20180101/stations.csv 
table name is stations 
schema option is Edit as text 
write the following in the schema textbox 
station_id:STRING,name:STRING,region_id:STRING,capacity:INTEGER 
create table 
NOTE: CHECK THE STATIONS TABLE TO SEE IF THE TABLE EXISTS NOW 

Create a BigQuery data mart 
business users will access data from data marts 
create a new dataset called name dm_regional_manager
TASK: BUSINESS USER, WANTS TO KNOW THE TOP TWO REGION IDs ordered by total stations capacity in that region. 

Now two options for the query results 
1. Create a table 
2. Create a view 

both tables and views have merits and demerits 
two merits of using a view 
1. View costs no additional storage, just saves SQL formula 
2. Real time, if you access the view then every time the underlying table changes, the view will get the latest update.

drawbacks to views 
1. views can be heavy,and when the underlying tables are large and there are many joins
and aggregations in the view's query, you may end up having very heavy processing

CONSIDER THIS SCENARIO 
Imagine you have 5 upstream raw tables, each 1 PB in size, and your downstream consists of 1,000 views 
accessing the 5 tables. You may end up processing the PBs of data repeatedly, and that's bad in terms of 
both cost and performance.

HOWEVER IN THIS SCENARIO => THE TABLE IS SMALL SO USE VIEWS 
CHECK createview.sql and follow instructions 
save the results of the query in csv file 


DATAWAREHOUSE IN BIGQUERY 
SCENARIO 2 
SOME REQUIREMENTS 
Access the following information 
1. How many bike trips take place daily? 
2. what is the daily average trip duratoon ? 
3. The top 5 station names as the starting station that has the longest trip duration 
4. The top 5 region names that have the shortest total trip durations 

THE BIKE TRIPS DATA IS IN THE GCS BUCKET AND EACH BUCKET FOLDER CONTAINS DAILY DATA 
THE REGIONS DATA IS FROM THE BIGQUERY PUBLIC DATASET 
NEW DATA WILL BE UPDATED DAILY IN THE GCS BUCKET FOR stations and trips table 

NOW ASK THE SAME QUESTIONS 
1. WHAT WILL YOU DO ? 
2. WHAT SERVICES WILL YOU USE ? 
3. HOW WILL YOU DO IT ?

IN THIS SCENARIO, WE WILL BE USING THE PYTHON API INSTEAD OF USING THE GCP CONSOLE 
CONSOLE IS NOT SCALABLE IN ENGINEERING PERSPECTIVE 
IE NEED TO CREATE MULTIPLE ETL PIPELINES 
BETTER TO HAVE LOOPS IN THE PIPELINES USING CODE 
ALSO MAKES IT EASY TO TEST AND DEPLOY 


SOME INITIAL THOUGHTS AND PLANNING 
1. SINCE THE USER IS INTERESTED IN DAILY MEASUREMENTS, WE WILL CREATE A LAYER FOR DAILY AGGREGATIONS TO THE USER. 
2. THERE WILL BE NEW DATA DAILY, SO WE NEED TO PLAN HOW TO HANDLE THE INCOMING DATA FROM GCS DIRECTORIES 
3. IMAGINE IF THREE TABLES, STATIONS, REGIONS, AND TRIPS IN REAL LIFE, IF THEY ARE DIFFERENT ENTITIES 
STATIONS AND REGIONS ARE STATIC OBJECTS WHILE TRIPS ARE EVENTS 
4. THE DATAMART IS SIMILAR TO SCENARIO 1 

FIVE PRINCIPAL STEPS 
1. CREATE REQUIRED DATASETS 
2. LOAD THE INITIAL TRIPS AND REGION TABLES TO BIGQUERY: 
--- TRIPS FROM GCS 
--- REGIONS FROM THE BIGQUERY PUBLIC DATASET 
3. HANDLE THE DAILY BATCH DATA LOADING 
--- FOR TRIPS TABLE 
--- FOR STATIONS TABLE 
4. DESIGN DATA MODELLING FOR BIGQUERY 
5. STORE THE BUSINESS QUESTIONS RESULT IN TABLES 
THERE ARE TWO DIFFERENT DATES IN THIS. 
INITIAL LOAD IS USING 2018-01-01 
HANDLE NEW DATA FROM 2018-01-02 AND UPCOMING DAYS WITHOUT ANY ISSUES 

LETS CREATE THE DATASETS USING PYTHON 
1. add a new dataset called dwh_bikesharing 
2. add other datasets => raw_bikesharing and dm_bikesharing 
(name the file createdatasets.py) and check the file 
upload the code to github dn update the codebase in editor and run the code 

NOW LOADING THE TRIPS INTO BIGQUERY 
=> create a python file to load the trips data 
=> loadtripsdata.py 
=> TRIPS HAS SUCCESSFULLY BEEN LOADED 

NOW LOAD THE REGIONS TABLE INTO BIGQUERY 
=> create python file to load the trips data 
=> loadregions.py 
=> QUERY IS SUCCESSFULL 

HANDLE DAILY BATCH DATA LOADING FOR THE TRIPS TABLE 
=> create python file to load trips 
=> loadtripsdata1.py 
some notes about this 
The code will append new records to the trips table. This kind of data is called events. 
In events data, every new record is a new event, which won't affect any existing data. 
The nature of an event, once it happens, can't be updated and deleted. 
This is similar to the real world; you can't change something that has happened.
WRITE_APPEND is a natural way to load event data.
=> RUN FILE SUCCESSFULL 
=> LETS RUN THIS SQL QUERY TO CHECK WHETHER RECORDS HAVE BE DUPLICATED OR NOT 
SELECT count(*) cnt_trip_id, trip_id
FROM `[your project id].raw_bikesharing.trips`
GROUP BY trip_id
HAVING cnt_trip_id > 1;

HANDLE BATCH DATA LOADING FOR STATIONS TABLE 
stations table, the approach will be different compared to the trips table. 
This kind of table may have new records (INSERT), updated records (UPDATE), 
and removed records (DELETE) records.
Imagine a real-world bike station; a new bike station can be demolished, 
capacities or the station name may change, or a new station may be established 
in the region. So, this kind of data is called a snapshot. 
Snapshot data is not event data. Snapshot data is an object or entity in the 
real world; it doesn't happen, it's just there.
WE WILL BE USING WRITE_TRUNCATE SO WE GET 
# If the table already exists, BigQuery overwrites the table data and 
uses the schema from the load.
=> create a python file 
=> loadstations.py 
=> LOAD SUCCESSFULL 
USE THIS QUERY TO CHECK IF THERE ARE ANY DUPLICATED RECORDS 
SELECT
station_id, count(*) as cnt_station
FROM `[your project id].raw_bikesharing.stations`
GROUP BY station_id
HAVING cnt_station > 1;
NO DATA WILL RETURN IF SUCCESSFULL 


HOW TO KEEP HISTORICAL RECORDS WHILE MAINTAINING VALID INFORMATION 
1. AFTER LOADING TO OUR TABLE IN raw_bikesharing, we will create another table that adds an 
insert_date column (or you can add it as an ETL process, before loading to BigQuery). 
The table will keep appending the new data daily. There will be duplication at the station_id level,
but since we have the insert_date column, we can use the date information to get just the latest data snapshot
2. Create a view that excludes the insert_date column and filter insert_date
using the CURRENT_DATE() function, which returns today's date.
3. The later user will access the view instead of the raw or historical tables. The user experience 
will still be the same since they can use the common query SELECT * FROM stations and 
obtain just today's station version.

With this mechanism, any time business users have requests for data from historical periods, 
you can always access the stations_history table.

DRAW BACK => AFFECT STORAGE AND PERFORMANCE 

OTHER APPROACHES 
1. Incremental load using the MERGE BigQuery operation. MERGE is a unique operation in BigQuery. 
It's a combination of INSERT if it does not exist, and UPDATE if there are changes.
2. Slowly changing dimensions or SCDs for short, 
SCDs are methods that you can follow to handle dimension data.


DESIGNING DATA MODELLING FOR BIGQUERY 
SOME ISSUES IF DATA WAREHOUSING PRINCIPLES ARE IGNORED 
1. DATA IS DUPLICATED IN MANY LOCATIONS 
2. SOME VALUES ARE NOT CONSISTENT ACROSS DIFFERENT USERS 
3. THE COST OF PROCESSING IS HIGHLY INEFFICIENT 
4. THE END USER DOESNT UNDERSTAND HOW TO USE THE DATAWAREHOUSE OBJECTS 
5. THE BUSINESS DOESNT TRUST THE DATA 

DATA MODELLING ?? 
Process of representing the database objects in our real world or business perspective 
Objects in bigquery can be datasets, tables or views. 
Some of the most common end users are business analysts, data analysts, data scientists, BI users, 
or any other roles that require access to the data for business purposes.

DESIGNING DATA MODEL IN DATA WAREHOUSE 
INMON VS KIMBALL METHODOLOGY 
INMON => DATA DRIVEN 
KIMBAL => USER DRIVEN 

INMON => CENTRAL DATAWAREHOUSE WITH A SINGLE SOURCE OF TRUTH 
DATA MODEL MUST BE NORMALISED TO THE LOWEST LEVEL SO DATA CAN BE HIGHLY CONSISTENT 
INMON MODEL FOLLOWS A TOP DOWN APPROACH 
DATAWAREHOUSE IS BUILT AS A SINGLE SOURCE OF TRUTH FOR ALL DOWNSTREAM DATA MARTS 
SOMETIMES SEEN AS ENTERPRISE DATA WAREHOUSE 

KIMBALL FOCUSES ON ANSWERING USER QUESTIONS AND FOLLOWS A BOTTOM UP APPROACH 
THIS APPROACH KEEPS THE USER QUESTIONS IN MIND AND USES THE QUESTIONS AS A BASIS 
TO BUILD NECESSARY TABLES. 
THERE IS THE FACT TABLE AND DIMENSION TABLE 
FACT TABLE => COLLECTION OF MEASUREMENTS OR METRICS IN A PREDEFINED GRANULARITY 
DIMENSION TABLE => COLLECTION OF ENTITY ATTRIBUTES THAT SUPPORT THE FACT TABLE 

CREATING A FACT TABLE 
=> create loadfacttabledailytrips.py 
=> run the file with some parameters date in the format yyyy-mm-dd 
=> so need to provide one when calling the Python command like this 
=> python loadfacttabledailytrips.py 2018-01-01
=> run it again python loadfacttabledailytrips.py 2018-01-02
=> HAVING ISSUES and dot it again 


ALTERNATIVE DATA MODEL USING NESTED DATA TYPES 
GOOGLE NORMALISED VS DENORMALISED DATA 
MOVING ON 
LOOKING AT at our bike-sharing region and station tables.
THE ORIGINAL DATA IS DENORMALISED 
And in our station table, each region has one or more stations.
Looking at our dimension table in the dwh dataset, we decided to 
denormalize the table to meet the star schema rule.
The rule is that you can't have parent tables for a dimension table, or, 
in other words, you can't join dimension tables to other dimension tables:
since one region can have one or more stations, 
we can store the stations as nested information under regions.
QUERY TO CREATE IT 

CREATE OR REPLACE TABLE `dwh_bikesharing.dim_stations_nested`
AS
SELECT
      regions.region_id,
      regions.name as region_name,
      ARRAY_AGG(stations) as stations
FROM
`packt-data-eng-on-gcp.raw_bikesharing.regions` regions
JOIN
`packt-data-eng-on-gcp.raw_bikesharing.stations` stations
ON CAST(regions.region_id AS STRING) = stations.region_id
GROUP BY regions.region_id, 
regions.name; 


The downside of using nested tables is that it's not easy to digest for common SQL users. 
Business users who are familiar with SQL might get confused with it the first time.


STORE THE BUSINESS QUESTIONS RESULT IN TABLES 
=> FACT IS TO ANSWER BUSINESS QUESTIONS WITH OUR FACT AND DIMENSIONS TABLE 

1. How many bike trips take place daily ? 
CREATE VIEW dm_operational.bike_trips_daily
AS
SELECT trip_date, SUM(total_trips) as total_trips_daily
FROM dwh_bikesharing.fact_trips_daily
GROUP BY trip_date;

2. What is the daily average trip duration ?? 
CREATE VIEW dm_operational.daily_avg_trip_duration
AS
SELECT trip_date, ROUND(AVG(avg_duration_sec)) as daily_
average_duration_sec
FROM dwh_bikesharing.fact_trips_daily
GROUP BY trip_date;

3.What are the top five station names of starting stations with the longest trip duration?
CREATE VIEW dm_operational.top_5_station_by_longest_
duration
AS
SELECT trip_date,  station_name, sum_duration_sec
FROM dwh_bikesharing.fact_trips_daily
JOIN dwh_bikesharing.dim_stations
ON start_station_id = station_id
WHERE trip_date = '2018-01-02'
ORDER BY sum_duration_sec desc
LIMIT 5;

4. What are the top three region names that have the shortest total trip durations?
CREATE VIEW dm_operational.top_3_region_by_shortest_duration
AS
SELECT trip_date, region_name, SUM(sum_duration_sec) as
total_sum_duration_sec
FROM dwh_bikesharing.fact_trips_daily
JOIN dwh_bikesharing.dim_stations
ON start_station_id = station_id
WHERE trip_date = '2018-01-02'
GROUP BY trip_date, region_name
ORDER BY total_sum_duration_sec asc
LIMIT 3;